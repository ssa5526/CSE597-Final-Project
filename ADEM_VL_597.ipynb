{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#cloning into repo\n",
        "!git clone https://github.com/Hao840/ADEM-VL.git\n",
        "%cd ADEM-VL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyjGbddjQBCz",
        "outputId": "c5998b6b-1ee2-41f2-989e-e0e76cfb2aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ADEM-VL'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 43 (delta 9), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (43/43), 1.34 MiB | 7.97 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "/content/ADEM-VL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs('./data', exist_ok=True)\n",
        "os.makedirs('./data/weights/7B', exist_ok=True)\n"
      ],
      "metadata": {
        "id": "lEk7EKAbRhiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#REMEMBER TO UPLOAD SCIENCEQA DATA TO DATA\n",
        "\n",
        "import zipfile\n",
        "\n",
        "zip_files = ['./data/train.zip', './data/val.zip', './data/test.zip']\n",
        "for zip_file in zip_files:\n",
        "    if zipfile.is_zipfile(zip_file):\n",
        "        print(f\"{zip_file} is a valid zip file.\")\n",
        "    else:\n",
        "        print(f\"{zip_file} is NOT a valid zip file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvhl7Y1gSR8L",
        "outputId": "4d7f124b-c4bc-4394-cc56-90bd9abf9d17"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data/train.zip is a valid zip file.\n",
            "./data/val.zip is a valid zip file.\n",
            "./data/test.zip is a valid zip file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Ensure the 'images' directory exists\n",
        "os.makedirs('./data/images', exist_ok=True)\n",
        "\n",
        "# Define the ZIP file paths\n",
        "zip_files = ['./data/train.zip']#, './data/val.zip', './data/test.zip']\n",
        "\n",
        "# Extract each ZIP file into the 'images' directory\n",
        "for zip_file in zip_files:\n",
        "    try:\n",
        "        print(f\"Extracting {zip_file} into ./data/images/...\")\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall('./data/images/')\n",
        "        print(f\"Extraction of {zip_file} completed!\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: {zip_file} is not a valid ZIP file or is corrupted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while extracting {zip_file}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luQ7JOdKRXf5",
        "outputId": "d6fa0fd2-8545-4b16-83a2-1a73611bea0b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/train.zip into ./data/images/...\n",
            "Extraction of ./data/train.zip completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#copy llama over\n",
        "!cp /content/drive/MyDrive/data/weights/tokenizermodel/7B/params.json ./data/weights/7B/\n",
        "!cp /content/drive/MyDrive/data/weights/tokenizermodel/7B/consolidated.00.pth ./data/weights/7B/\n",
        "!cp /content/drive/MyDrive/data/weights/tokenizer.model ./data/weights\n",
        "!cp /content/drive/MyDrive/data/weights/tokenizer.model ./data/weights/7B/"
      ],
      "metadata": {
        "id": "GLH-jD0Md_IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v1tH-hZlPH-q",
        "outputId": "9fefacca-a103-4b40-f054-6e9e46e6a807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchvision==0.14.1\n",
            "  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.12.2)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1) (11.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1) (2024.8.30)\n",
            "Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1 torchvision-0.14.1\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.13.1)\n",
            "Collecting sentencepiece==0.1.99 (from -r requirements.txt (line 2))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting timm==0.6.5 (from -r requirements.txt (line 3))\n",
            "  Downloading timm-0.6.5-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy (from -r requirements.txt (line 4))\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2024.9.11)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->-r requirements.txt (line 1)) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->-r requirements.txt (line 1)) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->-r requirements.txt (line 1)) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->-r requirements.txt (line 1)) (11.7.99)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.6.5->-r requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r requirements.txt (line 1)) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r requirements.txt (line 1)) (0.45.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->-r requirements.txt (line 4)) (0.2.13)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 6)) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 6)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 6)) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.6.5->-r requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.6.5->-r requirements.txt (line 3)) (11.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.6.5->-r requirements.txt (line 3)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.6.5->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.6.5->-r requirements.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.6.5->-r requirements.txt (line 3)) (2024.8.30)\n",
            "Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.6.5-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.8/512.8 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece, ftfy, timm\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.2.0\n",
            "    Uninstalling sentencepiece-0.2.0:\n",
            "      Successfully uninstalled sentencepiece-0.2.0\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.12\n",
            "    Uninstalling timm-1.0.12:\n",
            "      Successfully uninstalled timm-1.0.12\n",
            "Successfully installed ftfy-6.3.1 sentencepiece-0.1.99 timm-0.6.5\n"
          ]
        }
      ],
      "source": [
        "# Install necessary dependencies\n",
        "!pip install torch==1.13.1 torchvision==0.14.1\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing pycocoevalcap\n",
        "!pip install git+https://github.com/salaniz/pycocoevalcap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktOBNn76c0cM",
        "outputId": "da15529b-87bc-4658-c2ab-75894b6a0fb7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/salaniz/pycocoevalcap\n",
            "  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-nafa6z_1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-nafa6z_1\n",
            "  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap==1.2) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.16.0)\n",
            "Building wheels for collected packages: pycocoevalcap\n",
            "  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312245 sha256=9c4fc30f76dda663cab1ee00dff73c6b1ea32f568a04fa0072d016a27616d608\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w2gbwdb8/wheels/43/54/73/3e2c6d4ace7657958cde52ac6fd47b342cd4aae5a7aa4fcbf9\n",
            "Successfully built pycocoevalcap\n",
            "Installing collected packages: pycocoevalcap\n",
            "Successfully installed pycocoevalcap-1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun --nproc_per_node 1 train.py \\\n",
        "  --data_root ./data/ \\\n",
        "  --clip_root ./data/weights/clip/ \\\n",
        "  --caption_file ./data/captions.json \\\n",
        "  --output_dir ./data/weights/7B \\\n",
        "  --llama_model_path ./data/weights/ \\\n",
        "  --llm_model 7B \\\n",
        "  --max_seq_len 512 \\\n",
        "  --batch_size 4 \\\n",
        "  --accum_iter 8 \\\n",
        "  --epochs 5 \\\n",
        "  --warmup_epochs 0 \\\n",
        "  --blr 9e-3 \\\n",
        "  --weight_decay 0.01 \\\n",
        "  --adapter_dim 12 \\\n",
        "  --alpha 0.1 \\\n",
        "  --beta 0.01 \\\n",
        "  --drop_ratio 0.1 \\\n",
        "  --down_sample_num 256 64 \\\n",
        "  --dataset sqa\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd2pwfssaf3Y",
        "outputId": "8c1f73fe-f409-463c-eafd-6f0ee7c0a1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| distributed init (rank 0): env://, gpu 0\n",
            "[21:12:44.470262] job dir: /content/ADEM-VL\n",
            "[21:12:44.470408] Namespace(llama_model_path='./data/weights/',\n",
            "dataset='sqa',\n",
            "data_root='./data/',\n",
            "clip='ViT-L/14',\n",
            "clip_root='./data/weights/clip/',\n",
            "llm_model='7B',\n",
            "output_dir='./data/weights/7B',\n",
            "log_dir='./output_dir',\n",
            "prompt_format='CQM-A',\n",
            "options=['A',\n",
            "'B',\n",
            "'C',\n",
            "'D',\n",
            "'E'],\n",
            "caption_file='./data/captions.json',\n",
            "use_caption=False,\n",
            "num_workers=10,\n",
            "pin_mem=True,\n",
            "adapter_dim=12,\n",
            "hidden_proj=128,\n",
            "max_seq_len=512,\n",
            "seed=42,\n",
            "resume='',\n",
            "down_sample_num=[256,\n",
            "64],\n",
            "alpha=0.1,\n",
            "beta=0.01,\n",
            "drop_ratio=0.1,\n",
            "no_cls=False,\n",
            "epochs=5,\n",
            "start_epoch=0,\n",
            "batch_size=4,\n",
            "accum_iter=8,\n",
            "lr=None,\n",
            "blr=0.009,\n",
            "min_lr=0.0,\n",
            "warmup_epochs=0.0,\n",
            "weight_decay=0.01,\n",
            "clip_grad=None,\n",
            "device='cuda',\n",
            "cpu_load=False,\n",
            "gradient_checkpointing=False,\n",
            "world_size=1,\n",
            "local_rank=-1,\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "rank=0,\n",
            "gpu=0,\n",
            "distributed=True,\n",
            "dist_backend='nccl')\n",
            "[21:12:44.695425] Reloaded SentencePiece model from ./data/weights//tokenizer.model\n",
            "[21:12:44.695497] #words: 32000 - BOS ID: 1 - EOS ID: 2\n",
            "[21:12:44.703013] number of problems in split train: 12726\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "[21:12:44.704203] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fdf938f97b0>\n",
            "2024-12-10 21:12:44.920454: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-10 21:12:44.938750: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-10 21:12:44.960152: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-10 21:12:44.966528: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-10 21:12:44.981507: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-10 21:12:46.274582: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[21:12:47.550814] Reloaded SentencePiece model from data/weights/tokenizer.model\n",
            "[21:12:47.550885] #words: 32000 - BOS ID: 1 - EOS ID: 2\n",
            "[21:12:47.550913] Using model path: ./data/weights/, model_name: 7B\n",
            "[21:13:05.131986] ['adapter_emb1', 'adapter_emb2', 'backbone.visual.transformer.resblocks.0.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.0.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.1.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.1.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.2.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.2.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.3.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.3.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.4.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.4.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.5.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.5.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.6.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.6.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.7.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.7.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.8.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.8.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.9.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.9.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.10.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.10.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.11.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.11.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.12.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.12.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.13.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.13.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.14.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.14.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.15.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.15.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.16.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.16.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.17.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.17.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.18.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.18.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.19.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.19.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.20.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.20.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.21.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.21.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.22.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.22.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.23.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.23.adapter_mlp.fc2.weight', 'adapter_proj.fc1.weight', 'adapter_proj.fc1.bias', 'adapter_proj.fc2.weight', 'adapter_proj.fc2.bias', 'adapter_proj_cls.fc1.weight', 'adapter_proj_cls.fc1.bias', 'adapter_proj_cls.fc2.weight', 'adapter_proj_cls.fc2.bias']\n",
            "[21:13:05.132092]   + Number of trainable params: 4.53M\n",
            "[21:13:05.939866] base lr: 9.00e-03\n",
            "[21:13:05.939924] actual lr: 1.12e-03\n",
            "[21:13:05.939937] accumulate grad iterations: 8\n",
            "[21:13:05.939955] effective batch size: 32\n",
            "[21:13:05.939967] 0\n",
            "[21:13:05.985434] AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    capturable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    lr: 0.001125\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    capturable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    lr: 0.001125\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")\n",
            "[21:13:05.985567] Start training for 5 epochs\n",
            "[21:13:05.988177] log_dir: ./output_dir\n",
            "[21:13:08.334552] Epoch: [0]  [   0/3181]  eta: 2:04:19  lr: 0.001125  closs: 3.1074 (3.1074)  time: 2.3449  data: 0.5587  max mem: 21541\n",
            "[21:13:34.116477] Epoch: [0]  [ 100/3181]  eta: 0:14:17  lr: 0.001125  closs: 0.1732 (1.0255)  time: 0.2529  data: 0.0002  max mem: 31720\n",
            "[21:13:58.819213] Epoch: [0]  [ 200/3181]  eta: 0:13:03  lr: 0.001125  closs: 0.1678 (0.5989)  time: 0.2459  data: 0.0002  max mem: 31720\n",
            "[21:14:23.879755] Epoch: [0]  [ 300/3181]  eta: 0:12:25  lr: 0.001124  closs: 0.1544 (0.4531)  time: 0.2415  data: 0.0002  max mem: 31720\n",
            "[21:14:48.611638] Epoch: [0]  [ 400/3181]  eta: 0:11:51  lr: 0.001123  closs: 0.1506 (0.3787)  time: 0.2494  data: 0.0002  max mem: 31720\n",
            "[21:15:13.807376] Epoch: [0]  [ 500/3181]  eta: 0:11:23  lr: 0.001122  closs: 0.1423 (0.3311)  time: 0.2424  data: 0.0002  max mem: 31720\n",
            "[21:15:39.449589] Epoch: [0]  [ 600/3181]  eta: 0:10:58  lr: 0.001121  closs: 0.1227 (0.2982)  time: 0.2589  data: 0.0002  max mem: 31720\n",
            "[21:16:04.962008] Epoch: [0]  [ 700/3181]  eta: 0:10:33  lr: 0.001120  closs: 0.1406 (0.2755)  time: 0.2575  data: 0.0002  max mem: 31720\n",
            "[21:16:30.576083] Epoch: [0]  [ 800/3181]  eta: 0:10:08  lr: 0.001118  closs: 0.1160 (0.2564)  time: 0.2502  data: 0.0002  max mem: 31720\n",
            "[21:16:55.924935] Epoch: [0]  [ 900/3181]  eta: 0:09:42  lr: 0.001116  closs: 0.1207 (0.2426)  time: 0.2504  data: 0.0002  max mem: 32821\n",
            "[21:17:20.931894] Epoch: [0]  [1000/3181]  eta: 0:09:15  lr: 0.001114  closs: 0.1052 (0.2303)  time: 0.2525  data: 0.0002  max mem: 32821\n",
            "[21:17:45.739120] Epoch: [0]  [1100/3181]  eta: 0:08:48  lr: 0.001112  closs: 0.1130 (0.2206)  time: 0.2468  data: 0.0002  max mem: 32821\n",
            "[21:18:10.548493] Epoch: [0]  [1200/3181]  eta: 0:08:22  lr: 0.001109  closs: 0.1282 (0.2117)  time: 0.2492  data: 0.0002  max mem: 32821\n",
            "[21:18:36.086268] Epoch: [0]  [1300/3181]  eta: 0:07:57  lr: 0.001107  closs: 0.1129 (0.2044)  time: 0.2554  data: 0.0002  max mem: 32821\n",
            "[21:19:00.908864] Epoch: [0]  [1400/3181]  eta: 0:07:31  lr: 0.001104  closs: 0.0843 (0.1968)  time: 0.2458  data: 0.0002  max mem: 32821\n",
            "[21:19:26.049658] Epoch: [0]  [1500/3181]  eta: 0:07:05  lr: 0.001101  closs: 0.0923 (0.1902)  time: 0.2620  data: 0.0002  max mem: 32821\n",
            "[21:19:51.127631] Epoch: [0]  [1600/3181]  eta: 0:06:40  lr: 0.001097  closs: 0.0997 (0.1842)  time: 0.2451  data: 0.0002  max mem: 32821\n",
            "[21:20:16.302980] Epoch: [0]  [1700/3181]  eta: 0:06:14  lr: 0.001094  closs: 0.0742 (0.1794)  time: 0.2613  data: 0.0002  max mem: 32821\n",
            "[21:20:41.423351] Epoch: [0]  [1800/3181]  eta: 0:05:49  lr: 0.001090  closs: 0.0862 (0.1742)  time: 0.2587  data: 0.0002  max mem: 32821\n",
            "[21:21:06.991046] Epoch: [0]  [1900/3181]  eta: 0:05:24  lr: 0.001086  closs: 0.0792 (0.1699)  time: 0.2571  data: 0.0002  max mem: 32821\n",
            "[21:21:32.234749] Epoch: [0]  [2000/3181]  eta: 0:04:58  lr: 0.001082  closs: 0.0607 (0.1653)  time: 0.2454  data: 0.0002  max mem: 32821\n",
            "[21:21:57.235067] Epoch: [0]  [2100/3181]  eta: 0:04:33  lr: 0.001077  closs: 0.0663 (0.1619)  time: 0.2516  data: 0.0002  max mem: 32821\n",
            "[21:22:22.527931] Epoch: [0]  [2200/3181]  eta: 0:04:08  lr: 0.001073  closs: 0.0808 (0.1587)  time: 0.2652  data: 0.0002  max mem: 32821\n",
            "[21:22:48.310012] Epoch: [0]  [2300/3181]  eta: 0:03:42  lr: 0.001068  closs: 0.0753 (0.1559)  time: 0.2427  data: 0.0002  max mem: 32821\n",
            "[21:23:13.844604] Epoch: [0]  [2400/3181]  eta: 0:03:17  lr: 0.001063  closs: 0.0889 (0.1529)  time: 0.2524  data: 0.0002  max mem: 32821\n",
            "[21:23:39.086102] Epoch: [0]  [2500/3181]  eta: 0:02:52  lr: 0.001058  closs: 0.0657 (0.1502)  time: 0.2592  data: 0.0002  max mem: 32821\n",
            "[21:24:04.329655] Epoch: [0]  [2600/3181]  eta: 0:02:27  lr: 0.001052  closs: 0.0674 (0.1473)  time: 0.2549  data: 0.0002  max mem: 32821\n",
            "[21:24:29.718873] Epoch: [0]  [2700/3181]  eta: 0:02:01  lr: 0.001047  closs: 0.0756 (0.1448)  time: 0.2444  data: 0.0002  max mem: 32821\n",
            "[21:24:54.597929] Epoch: [0]  [2800/3181]  eta: 0:01:36  lr: 0.001041  closs: 0.0865 (0.1424)  time: 0.2486  data: 0.0002  max mem: 32821\n",
            "[21:25:20.273888] Epoch: [0]  [2900/3181]  eta: 0:01:11  lr: 0.001035  closs: 0.0823 (0.1405)  time: 0.2586  data: 0.0002  max mem: 32821\n",
            "[21:25:45.516881] Epoch: [0]  [3000/3181]  eta: 0:00:45  lr: 0.001029  closs: 0.0601 (0.1382)  time: 0.2460  data: 0.0002  max mem: 32821\n",
            "[21:26:11.260840] Epoch: [0]  [3100/3181]  eta: 0:00:20  lr: 0.001023  closs: 0.0541 (0.1358)  time: 0.2472  data: 0.0002  max mem: 32821\n",
            "[21:26:31.443214] Epoch: [0]  [3180/3181]  eta: 0:00:00  lr: 0.001018  closs: 0.0623 (0.1342)  time: 0.2474  data: 0.0002  max mem: 32821\n",
            "[21:26:31.571216] Epoch: [0] Total time: 0:13:25 (0.2532 s / it)\n",
            "[21:26:31.572080] Averaged stats: lr: 0.001018  closs: 0.0623 (0.1342)\n",
            "[21:26:31.676582] log_dir: ./output_dir\n",
            "[21:26:32.542602] Epoch: [1]  [   0/3181]  eta: 0:45:50  lr: 0.001018  closs: 0.0646 (0.0646)  time: 0.8646  data: 0.5901  max mem: 32821\n",
            "[21:26:57.824617] Epoch: [1]  [ 100/3181]  eta: 0:13:17  lr: 0.001011  closs: 0.0767 (0.0677)  time: 0.2561  data: 0.0002  max mem: 32821\n",
            "[21:27:23.220102] Epoch: [1]  [ 200/3181]  eta: 0:12:44  lr: 0.001004  closs: 0.0780 (0.0699)  time: 0.2560  data: 0.0002  max mem: 32821\n",
            "[21:27:48.694723] Epoch: [1]  [ 300/3181]  eta: 0:12:17  lr: 0.000997  closs: 0.0354 (0.0682)  time: 0.2591  data: 0.0002  max mem: 32821\n",
            "[21:28:14.336722] Epoch: [1]  [ 400/3181]  eta: 0:11:51  lr: 0.000990  closs: 0.0886 (0.0686)  time: 0.2465  data: 0.0002  max mem: 32821\n",
            "[21:28:39.742082] Epoch: [1]  [ 500/3181]  eta: 0:11:25  lr: 0.000983  closs: 0.0429 (0.0667)  time: 0.2502  data: 0.0002  max mem: 32822\n",
            "[21:29:05.384211] Epoch: [1]  [ 600/3181]  eta: 0:11:00  lr: 0.000975  closs: 0.0443 (0.0664)  time: 0.2671  data: 0.0002  max mem: 32822\n",
            "[21:29:30.765266] Epoch: [1]  [ 700/3181]  eta: 0:10:33  lr: 0.000968  closs: 0.0695 (0.0669)  time: 0.2429  data: 0.0002  max mem: 32822\n",
            "[21:29:55.993466] Epoch: [1]  [ 800/3181]  eta: 0:10:07  lr: 0.000960  closs: 0.0466 (0.0670)  time: 0.2471  data: 0.0003  max mem: 32822\n",
            "[21:30:20.862747] Epoch: [1]  [ 900/3181]  eta: 0:09:40  lr: 0.000952  closs: 0.0641 (0.0670)  time: 0.2483  data: 0.0002  max mem: 32822\n",
            "[21:30:45.737193] Epoch: [1]  [1000/3181]  eta: 0:09:13  lr: 0.000944  closs: 0.0767 (0.0673)  time: 0.2490  data: 0.0003  max mem: 32822\n",
            "[21:31:10.896391] Epoch: [1]  [1100/3181]  eta: 0:08:47  lr: 0.000936  closs: 0.0698 (0.0671)  time: 0.2369  data: 0.0002  max mem: 32822\n",
            "[21:31:36.245133] Epoch: [1]  [1200/3181]  eta: 0:08:22  lr: 0.000927  closs: 0.0591 (0.0664)  time: 0.2502  data: 0.0002  max mem: 32822\n",
            "[21:32:01.158360] Epoch: [1]  [1300/3181]  eta: 0:07:56  lr: 0.000919  closs: 0.0610 (0.0660)  time: 0.2533  data: 0.0003  max mem: 32822\n",
            "[21:32:26.942432] Epoch: [1]  [1400/3181]  eta: 0:07:31  lr: 0.000910  closs: 0.0382 (0.0660)  time: 0.2692  data: 0.0002  max mem: 32822\n",
            "[21:32:52.484890] Epoch: [1]  [1500/3181]  eta: 0:07:06  lr: 0.000902  closs: 0.0547 (0.0660)  time: 0.2590  data: 0.0003  max mem: 32822\n",
            "[21:33:17.605305] Epoch: [1]  [1600/3181]  eta: 0:06:40  lr: 0.000892  closs: 0.0647 (0.0661)  time: 0.2516  data: 0.0002  max mem: 32822\n",
            "[21:33:42.567747] Epoch: [1]  [1700/3181]  eta: 0:06:15  lr: 0.000884  closs: 0.0439 (0.0658)  time: 0.2493  data: 0.0002  max mem: 32822\n",
            "[21:34:07.985880] Epoch: [1]  [1800/3181]  eta: 0:05:49  lr: 0.000874  closs: 0.0524 (0.0655)  time: 0.2690  data: 0.0002  max mem: 32822\n",
            "[21:34:33.262555] Epoch: [1]  [1900/3181]  eta: 0:05:24  lr: 0.000865  closs: 0.0409 (0.0650)  time: 0.2626  data: 0.0002  max mem: 32822\n",
            "[21:34:58.636551] Epoch: [1]  [2000/3181]  eta: 0:04:59  lr: 0.000855  closs: 0.0415 (0.0648)  time: 0.2588  data: 0.0002  max mem: 32822\n",
            "[21:35:24.044291] Epoch: [1]  [2100/3181]  eta: 0:04:33  lr: 0.000846  closs: 0.0366 (0.0644)  time: 0.2503  data: 0.0002  max mem: 32822\n",
            "[21:35:49.369356] Epoch: [1]  [2200/3181]  eta: 0:04:08  lr: 0.000836  closs: 0.0266 (0.0642)  time: 0.2540  data: 0.0002  max mem: 32822\n",
            "[21:36:14.177660] Epoch: [1]  [2300/3181]  eta: 0:03:43  lr: 0.000827  closs: 0.0496 (0.0640)  time: 0.2447  data: 0.0002  max mem: 32822\n",
            "[21:36:39.609032] Epoch: [1]  [2400/3181]  eta: 0:03:17  lr: 0.000816  closs: 0.0428 (0.0637)  time: 0.2557  data: 0.0002  max mem: 32822\n",
            "[21:37:04.795925] Epoch: [1]  [2500/3181]  eta: 0:02:52  lr: 0.000807  closs: 0.0440 (0.0636)  time: 0.2535  data: 0.0003  max mem: 32822\n",
            "[21:37:30.964442] Epoch: [1]  [2600/3181]  eta: 0:02:27  lr: 0.000796  closs: 0.0492 (0.0631)  time: 0.2486  data: 0.0002  max mem: 32822\n",
            "[21:37:56.735835] Epoch: [1]  [2700/3181]  eta: 0:02:01  lr: 0.000787  closs: 0.0378 (0.0632)  time: 0.2523  data: 0.0002  max mem: 32822\n",
            "[21:38:22.101498] Epoch: [1]  [2800/3181]  eta: 0:01:36  lr: 0.000776  closs: 0.0302 (0.0632)  time: 0.2565  data: 0.0002  max mem: 32822\n",
            "[21:38:47.204274] Epoch: [1]  [2900/3181]  eta: 0:01:11  lr: 0.000766  closs: 0.0420 (0.0630)  time: 0.2484  data: 0.0002  max mem: 32822\n",
            "[21:39:12.360072] Epoch: [1]  [3000/3181]  eta: 0:00:45  lr: 0.000755  closs: 0.0557 (0.0631)  time: 0.2542  data: 0.0002  max mem: 32822\n",
            "[21:39:38.141974] Epoch: [1]  [3100/3181]  eta: 0:00:20  lr: 0.000745  closs: 0.0595 (0.0633)  time: 0.2667  data: 0.0002  max mem: 32822\n",
            "[21:39:58.452515] Epoch: [1]  [3180/3181]  eta: 0:00:00  lr: 0.000737  closs: 0.0677 (0.0633)  time: 0.2561  data: 0.0002  max mem: 32822\n",
            "[21:39:58.572379] Epoch: [1] Total time: 0:13:26 (0.2537 s / it)\n",
            "[21:39:58.573293] Averaged stats: lr: 0.000737  closs: 0.0677 (0.0633)\n",
            "[21:39:58.677667] log_dir: ./output_dir\n",
            "[21:39:59.518168] Epoch: [2]  [   0/3181]  eta: 0:44:26  lr: 0.000736  closs: 0.0055 (0.0055)  time: 0.8383  data: 0.5575  max mem: 32822\n",
            "[21:40:24.905525] Epoch: [2]  [ 100/3181]  eta: 0:13:19  lr: 0.000726  closs: 0.0305 (0.0537)  time: 0.2499  data: 0.0002  max mem: 32822\n",
            "[21:40:50.739709] Epoch: [2]  [ 200/3181]  eta: 0:12:52  lr: 0.000715  closs: 0.0357 (0.0498)  time: 0.2648  data: 0.0002  max mem: 32822\n",
            "[21:41:15.603198] Epoch: [2]  [ 300/3181]  eta: 0:12:16  lr: 0.000705  closs: 0.0396 (0.0492)  time: 0.2482  data: 0.0002  max mem: 32822\n",
            "[21:41:41.361485] Epoch: [2]  [ 400/3181]  eta: 0:11:52  lr: 0.000694  closs: 0.0475 (0.0489)  time: 0.2569  data: 0.0002  max mem: 32822\n",
            "[21:42:06.308828] Epoch: [2]  [ 500/3181]  eta: 0:11:22  lr: 0.000683  closs: 0.0213 (0.0487)  time: 0.2440  data: 0.0002  max mem: 32822\n",
            "[21:42:31.027474] Epoch: [2]  [ 600/3181]  eta: 0:10:54  lr: 0.000672  closs: 0.0508 (0.0480)  time: 0.2551  data: 0.0002  max mem: 32822\n",
            "[21:42:56.225525] Epoch: [2]  [ 700/3181]  eta: 0:10:28  lr: 0.000661  closs: 0.0462 (0.0474)  time: 0.2539  data: 0.0002  max mem: 32822\n",
            "[21:43:21.531529] Epoch: [2]  [ 800/3181]  eta: 0:10:02  lr: 0.000650  closs: 0.0560 (0.0477)  time: 0.2479  data: 0.0002  max mem: 32822\n",
            "[21:43:47.100886] Epoch: [2]  [ 900/3181]  eta: 0:09:38  lr: 0.000639  closs: 0.0302 (0.0472)  time: 0.2664  data: 0.0002  max mem: 32822\n",
            "[21:44:12.464071] Epoch: [2]  [1000/3181]  eta: 0:09:12  lr: 0.000628  closs: 0.0684 (0.0469)  time: 0.2682  data: 0.0002  max mem: 32822\n",
            "[21:44:38.025975] Epoch: [2]  [1100/3181]  eta: 0:08:47  lr: 0.000617  closs: 0.0147 (0.0463)  time: 0.2492  data: 0.0002  max mem: 32822\n",
            "[21:45:03.246526] Epoch: [2]  [1200/3181]  eta: 0:08:22  lr: 0.000606  closs: 0.0487 (0.0465)  time: 0.2638  data: 0.0002  max mem: 32822\n",
            "[21:45:28.864267] Epoch: [2]  [1300/3181]  eta: 0:07:57  lr: 0.000595  closs: 0.0437 (0.0461)  time: 0.2700  data: 0.0002  max mem: 32822\n",
            "[21:45:53.694598] Epoch: [2]  [1400/3181]  eta: 0:07:31  lr: 0.000584  closs: 0.0249 (0.0459)  time: 0.2511  data: 0.0002  max mem: 32822\n",
            "[21:46:19.088450] Epoch: [2]  [1500/3181]  eta: 0:07:05  lr: 0.000573  closs: 0.0296 (0.0464)  time: 0.2546  data: 0.0002  max mem: 32822\n",
            "[21:46:43.980914] Epoch: [2]  [1600/3181]  eta: 0:06:40  lr: 0.000561  closs: 0.0379 (0.0463)  time: 0.2536  data: 0.0002  max mem: 32822\n",
            "[21:47:08.822681] Epoch: [2]  [1700/3181]  eta: 0:06:14  lr: 0.000551  closs: 0.0429 (0.0463)  time: 0.2389  data: 0.0002  max mem: 32822\n",
            "[21:47:34.973664] Epoch: [2]  [1800/3181]  eta: 0:05:49  lr: 0.000539  closs: 0.0196 (0.0466)  time: 0.2600  data: 0.0002  max mem: 32822\n",
            "[21:48:00.094919] Epoch: [2]  [1900/3181]  eta: 0:05:24  lr: 0.000529  closs: 0.0253 (0.0465)  time: 0.2522  data: 0.0002  max mem: 32822\n",
            "[21:48:25.148602] Epoch: [2]  [2000/3181]  eta: 0:04:58  lr: 0.000517  closs: 0.0458 (0.0469)  time: 0.2477  data: 0.0002  max mem: 32824\n",
            "[21:48:50.442073] Epoch: [2]  [2100/3181]  eta: 0:04:33  lr: 0.000506  closs: 0.0420 (0.0466)  time: 0.2476  data: 0.0002  max mem: 32824\n",
            "[21:49:15.836815] Epoch: [2]  [2200/3181]  eta: 0:04:08  lr: 0.000495  closs: 0.0400 (0.0465)  time: 0.2532  data: 0.0002  max mem: 32824\n",
            "[21:49:41.836706] Epoch: [2]  [2300/3181]  eta: 0:03:43  lr: 0.000484  closs: 0.0344 (0.0461)  time: 0.2621  data: 0.0002  max mem: 32824\n",
            "[21:50:06.786164] Epoch: [2]  [2400/3181]  eta: 0:03:17  lr: 0.000473  closs: 0.0224 (0.0463)  time: 0.2583  data: 0.0002  max mem: 32824\n",
            "[21:50:32.205632] Epoch: [2]  [2500/3181]  eta: 0:02:52  lr: 0.000462  closs: 0.0195 (0.0462)  time: 0.2449  data: 0.0002  max mem: 32824\n",
            "[21:50:57.245202] Epoch: [2]  [2600/3181]  eta: 0:02:27  lr: 0.000451  closs: 0.0160 (0.0463)  time: 0.2518  data: 0.0002  max mem: 32824\n",
            "[21:51:23.202378] Epoch: [2]  [2700/3181]  eta: 0:02:01  lr: 0.000441  closs: 0.0247 (0.0460)  time: 0.2649  data: 0.0002  max mem: 32824\n",
            "[21:51:49.055813] Epoch: [2]  [2800/3181]  eta: 0:01:36  lr: 0.000429  closs: 0.0249 (0.0460)  time: 0.2507  data: 0.0002  max mem: 32824\n",
            "[21:52:14.516332] Epoch: [2]  [2900/3181]  eta: 0:01:11  lr: 0.000419  closs: 0.0369 (0.0457)  time: 0.2445  data: 0.0002  max mem: 32824\n",
            "[21:52:40.014144] Epoch: [2]  [3000/3181]  eta: 0:00:45  lr: 0.000408  closs: 0.0221 (0.0453)  time: 0.2643  data: 0.0002  max mem: 32824\n",
            "[21:53:04.997581] Epoch: [2]  [3100/3181]  eta: 0:00:20  lr: 0.000398  closs: 0.0342 (0.0451)  time: 0.2519  data: 0.0002  max mem: 32824\n",
            "[21:53:25.529202] Epoch: [2]  [3180/3181]  eta: 0:00:00  lr: 0.000389  closs: 0.0559 (0.0452)  time: 0.2476  data: 0.0002  max mem: 32824\n",
            "[21:53:25.651199] Epoch: [2] Total time: 0:13:26 (0.2537 s / it)\n",
            "[21:53:25.652101] Averaged stats: lr: 0.000389  closs: 0.0559 (0.0452)\n",
            "[21:53:25.757957] log_dir: ./output_dir\n",
            "[21:53:26.605908] Epoch: [3]  [   0/3181]  eta: 0:44:53  lr: 0.000389  closs: 0.0831 (0.0831)  time: 0.8467  data: 0.5610  max mem: 32824\n",
            "[21:53:52.384101] Epoch: [3]  [ 100/3181]  eta: 0:13:32  lr: 0.000379  closs: 0.0353 (0.0332)  time: 0.2441  data: 0.0002  max mem: 32824\n",
            "[21:54:17.762017] Epoch: [3]  [ 200/3181]  eta: 0:12:51  lr: 0.000368  closs: 0.0003 (0.0332)  time: 0.2678  data: 0.0002  max mem: 32824\n",
            "[21:54:42.852678] Epoch: [3]  [ 300/3181]  eta: 0:12:17  lr: 0.000358  closs: 0.0097 (0.0328)  time: 0.2472  data: 0.0002  max mem: 32824\n",
            "[21:55:08.555666] Epoch: [3]  [ 400/3181]  eta: 0:11:52  lr: 0.000347  closs: 0.0184 (0.0329)  time: 0.2592  data: 0.0002  max mem: 32824\n",
            "[21:55:33.669417] Epoch: [3]  [ 500/3181]  eta: 0:11:24  lr: 0.000337  closs: 0.0361 (0.0338)  time: 0.2539  data: 0.0002  max mem: 32824\n",
            "[21:55:58.848875] Epoch: [3]  [ 600/3181]  eta: 0:10:57  lr: 0.000327  closs: 0.0224 (0.0334)  time: 0.2451  data: 0.0002  max mem: 32824\n",
            "[21:56:23.956155] Epoch: [3]  [ 700/3181]  eta: 0:10:30  lr: 0.000317  closs: 0.0262 (0.0335)  time: 0.2475  data: 0.0002  max mem: 32824\n",
            "[21:56:49.365225] Epoch: [3]  [ 800/3181]  eta: 0:10:05  lr: 0.000307  closs: 0.0253 (0.0340)  time: 0.2414  data: 0.0002  max mem: 32824\n",
            "[21:57:14.357931] Epoch: [3]  [ 900/3181]  eta: 0:09:38  lr: 0.000297  closs: 0.0228 (0.0342)  time: 0.2446  data: 0.0002  max mem: 32824\n",
            "[21:57:39.620433] Epoch: [3]  [1000/3181]  eta: 0:09:13  lr: 0.000287  closs: 0.0177 (0.0343)  time: 0.2549  data: 0.0002  max mem: 32824\n",
            "[21:58:04.594489] Epoch: [3]  [1100/3181]  eta: 0:08:46  lr: 0.000278  closs: 0.0182 (0.0350)  time: 0.2492  data: 0.0002  max mem: 32824\n",
            "[21:58:29.193107] Epoch: [3]  [1200/3181]  eta: 0:08:20  lr: 0.000268  closs: 0.0185 (0.0354)  time: 0.2403  data: 0.0002  max mem: 32824\n",
            "[21:58:54.215432] Epoch: [3]  [1300/3181]  eta: 0:07:54  lr: 0.000259  closs: 0.0400 (0.0354)  time: 0.2459  data: 0.0002  max mem: 32824\n",
            "[21:59:18.862297] Epoch: [3]  [1400/3181]  eta: 0:07:28  lr: 0.000249  closs: 0.0035 (0.0350)  time: 0.2442  data: 0.0002  max mem: 32824\n",
            "[21:59:43.986298] Epoch: [3]  [1500/3181]  eta: 0:07:03  lr: 0.000240  closs: 0.0217 (0.0349)  time: 0.2548  data: 0.0002  max mem: 32824\n",
            "[22:00:08.872991] Epoch: [3]  [1600/3181]  eta: 0:06:38  lr: 0.000231  closs: 0.0155 (0.0347)  time: 0.2472  data: 0.0002  max mem: 32824\n",
            "[22:00:34.191571] Epoch: [3]  [1700/3181]  eta: 0:06:12  lr: 0.000222  closs: 0.0283 (0.0343)  time: 0.2465  data: 0.0002  max mem: 32824\n",
            "[22:01:00.301579] Epoch: [3]  [1800/3181]  eta: 0:05:48  lr: 0.000213  closs: 0.0143 (0.0341)  time: 0.2481  data: 0.0002  max mem: 32824\n",
            "[22:01:26.132121] Epoch: [3]  [1900/3181]  eta: 0:05:23  lr: 0.000205  closs: 0.0095 (0.0339)  time: 0.2629  data: 0.0002  max mem: 32824\n",
            "[22:01:51.903031] Epoch: [3]  [2000/3181]  eta: 0:04:58  lr: 0.000196  closs: 0.0003 (0.0334)  time: 0.2618  data: 0.0002  max mem: 32824\n",
            "[22:02:17.110168] Epoch: [3]  [2100/3181]  eta: 0:04:33  lr: 0.000188  closs: 0.0252 (0.0337)  time: 0.2491  data: 0.0002  max mem: 32824\n",
            "[22:02:43.181262] Epoch: [3]  [2200/3181]  eta: 0:04:08  lr: 0.000180  closs: 0.0159 (0.0338)  time: 0.2579  data: 0.0002  max mem: 32824\n",
            "[22:03:08.701211] Epoch: [3]  [2300/3181]  eta: 0:03:43  lr: 0.000172  closs: 0.0206 (0.0338)  time: 0.2429  data: 0.0002  max mem: 32824\n",
            "[22:03:34.141616] Epoch: [3]  [2400/3181]  eta: 0:03:17  lr: 0.000164  closs: 0.0258 (0.0336)  time: 0.2467  data: 0.0002  max mem: 32824\n",
            "[22:03:58.779573] Epoch: [3]  [2500/3181]  eta: 0:02:52  lr: 0.000156  closs: 0.0000 (0.0331)  time: 0.2560  data: 0.0002  max mem: 32824\n",
            "[22:04:24.124980] Epoch: [3]  [2600/3181]  eta: 0:02:27  lr: 0.000148  closs: 0.0185 (0.0329)  time: 0.2587  data: 0.0002  max mem: 32824\n",
            "[22:04:49.205310] Epoch: [3]  [2700/3181]  eta: 0:02:01  lr: 0.000141  closs: 0.0305 (0.0330)  time: 0.2541  data: 0.0002  max mem: 32824\n",
            "[22:05:14.598205] Epoch: [3]  [2800/3181]  eta: 0:01:36  lr: 0.000134  closs: 0.0162 (0.0331)  time: 0.2467  data: 0.0002  max mem: 32824\n",
            "[22:05:39.804587] Epoch: [3]  [2900/3181]  eta: 0:01:11  lr: 0.000127  closs: 0.0096 (0.0330)  time: 0.2419  data: 0.0002  max mem: 32824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ./data/weights/7B/checkpoint-4.pth /content/drive/MyDrive/data/weights"
      ],
      "metadata": {
        "id": "F1JvG45BSdYk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python eval_sqa.py \\\n",
        "    --data_root ./data/ \\\n",
        "    --clip_root ./data/weights/clip/ \\\n",
        "    --llm_model 7B \\\n",
        "    --adapter_path ./data/weights/7B \\\n",
        "    --alpha 0.1 \\\n",
        "    --beta 0.01 \\\n",
        "    --drop_ratio 0.1 \\\n",
        "    --down_sample_num 256 64 \\\n",
        "    --batch_size 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zndCyxKpVbOY",
        "outputId": "ef8d5e76-849a-408d-bf32-8a6ae9418afa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloaded SentencePiece model from data/weights/tokenizer.model\n",
            "#words: 32000 - BOS ID: 1 - EOS ID: 2\n",
            "Using model path: ./data/weights/, model_name: 7B\n",
            "['adapter_emb1', 'adapter_emb2', 'backbone.visual.transformer.resblocks.0.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.0.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.1.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.1.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.2.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.2.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.3.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.3.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.4.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.4.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.5.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.5.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.6.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.6.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.7.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.7.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.8.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.8.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.9.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.9.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.10.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.10.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.11.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.11.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.12.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.12.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.13.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.13.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.14.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.14.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.15.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.15.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.16.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.16.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.17.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.17.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.18.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.18.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.19.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.19.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.20.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.20.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.21.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.21.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.22.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.22.adapter_mlp.fc2.weight', 'backbone.visual.transformer.resblocks.23.adapter_mlp.fc1.weight', 'backbone.visual.transformer.resblocks.23.adapter_mlp.fc2.weight', 'adapter_proj.fc1.weight', 'adapter_proj.fc1.bias', 'adapter_proj.fc2.weight', 'adapter_proj.fc2.bias', 'adapter_proj_cls.fc1.weight', 'adapter_proj_cls.fc1.bias', 'adapter_proj_cls.fc2.weight', 'adapter_proj_cls.fc2.bias']\n",
            "  + Number of trainable params: 4.53M\n",
            "_IncompatibleKeys(missing_keys=['tok_embeddings.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wv.weight', 'layers.0.attention.wo.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.attention_norm.weight', 'layers.0.ffn_norm.weight', 'layers.1.attention.wq.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wv.weight', 'layers.1.attention.wo.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.attention_norm.weight', 'layers.1.ffn_norm.weight', 'layers.2.attention.wq.weight', 'layers.2.attention.wk.weight', 'layers.2.attention.wv.weight', 'layers.2.attention.wo.weight', 'layers.2.feed_forward.w1.weight', 'layers.2.feed_forward.w2.weight', 'layers.2.feed_forward.w3.weight', 'layers.2.attention_norm.weight', 'layers.2.ffn_norm.weight', 'layers.3.attention.wq.weight', 'layers.3.attention.wk.weight', 'layers.3.attention.wv.weight', 'layers.3.attention.wo.weight', 'layers.3.feed_forward.w1.weight', 'layers.3.feed_forward.w2.weight', 'layers.3.feed_forward.w3.weight', 'layers.3.attention_norm.weight', 'layers.3.ffn_norm.weight', 'layers.4.attention.wq.weight', 'layers.4.attention.wk.weight', 'layers.4.attention.wv.weight', 'layers.4.attention.wo.weight', 'layers.4.feed_forward.w1.weight', 'layers.4.feed_forward.w2.weight', 'layers.4.feed_forward.w3.weight', 'layers.4.attention_norm.weight', 'layers.4.ffn_norm.weight', 'layers.5.attention.wq.weight', 'layers.5.attention.wk.weight', 'layers.5.attention.wv.weight', 'layers.5.attention.wo.weight', 'layers.5.feed_forward.w1.weight', 'layers.5.feed_forward.w2.weight', 'layers.5.feed_forward.w3.weight', 'layers.5.attention_norm.weight', 'layers.5.ffn_norm.weight', 'layers.6.attention.wq.weight', 'layers.6.attention.wk.weight', 'layers.6.attention.wv.weight', 'layers.6.attention.wo.weight', 'layers.6.feed_forward.w1.weight', 'layers.6.feed_forward.w2.weight', 'layers.6.feed_forward.w3.weight', 'layers.6.attention_norm.weight', 'layers.6.ffn_norm.weight', 'layers.7.attention.wq.weight', 'layers.7.attention.wk.weight', 'layers.7.attention.wv.weight', 'layers.7.attention.wo.weight', 'layers.7.feed_forward.w1.weight', 'layers.7.feed_forward.w2.weight', 'layers.7.feed_forward.w3.weight', 'layers.7.attention_norm.weight', 'layers.7.ffn_norm.weight', 'layers.8.attention.wq.weight', 'layers.8.attention.wk.weight', 'layers.8.attention.wv.weight', 'layers.8.attention.wo.weight', 'layers.8.feed_forward.w1.weight', 'layers.8.feed_forward.w2.weight', 'layers.8.feed_forward.w3.weight', 'layers.8.attention_norm.weight', 'layers.8.ffn_norm.weight', 'layers.9.attention.wq.weight', 'layers.9.attention.wk.weight', 'layers.9.attention.wv.weight', 'layers.9.attention.wo.weight', 'layers.9.feed_forward.w1.weight', 'layers.9.feed_forward.w2.weight', 'layers.9.feed_forward.w3.weight', 'layers.9.attention_norm.weight', 'layers.9.ffn_norm.weight', 'layers.10.attention.wq.weight', 'layers.10.attention.wk.weight', 'layers.10.attention.wv.weight', 'layers.10.attention.wo.weight', 'layers.10.feed_forward.w1.weight', 'layers.10.feed_forward.w2.weight', 'layers.10.feed_forward.w3.weight', 'layers.10.attention_norm.weight', 'layers.10.ffn_norm.weight', 'layers.11.attention.wq.weight', 'layers.11.attention.wk.weight', 'layers.11.attention.wv.weight', 'layers.11.attention.wo.weight', 'layers.11.feed_forward.w1.weight', 'layers.11.feed_forward.w2.weight', 'layers.11.feed_forward.w3.weight', 'layers.11.attention_norm.weight', 'layers.11.ffn_norm.weight', 'layers.12.attention.wq.weight', 'layers.12.attention.wk.weight', 'layers.12.attention.wv.weight', 'layers.12.attention.wo.weight', 'layers.12.feed_forward.w1.weight', 'layers.12.feed_forward.w2.weight', 'layers.12.feed_forward.w3.weight', 'layers.12.attention_norm.weight', 'layers.12.ffn_norm.weight', 'layers.13.attention.wq.weight', 'layers.13.attention.wk.weight', 'layers.13.attention.wv.weight', 'layers.13.attention.wo.weight', 'layers.13.feed_forward.w1.weight', 'layers.13.feed_forward.w2.weight', 'layers.13.feed_forward.w3.weight', 'layers.13.attention_norm.weight', 'layers.13.ffn_norm.weight', 'layers.14.attention.wq.weight', 'layers.14.attention.wk.weight', 'layers.14.attention.wv.weight', 'layers.14.attention.wo.weight', 'layers.14.feed_forward.w1.weight', 'layers.14.feed_forward.w2.weight', 'layers.14.feed_forward.w3.weight', 'layers.14.attention_norm.weight', 'layers.14.ffn_norm.weight', 'layers.15.attention.wq.weight', 'layers.15.attention.wk.weight', 'layers.15.attention.wv.weight', 'layers.15.attention.wo.weight', 'layers.15.feed_forward.w1.weight', 'layers.15.feed_forward.w2.weight', 'layers.15.feed_forward.w3.weight', 'layers.15.attention_norm.weight', 'layers.15.ffn_norm.weight', 'layers.16.attention.wq.weight', 'layers.16.attention.wk.weight', 'layers.16.attention.wv.weight', 'layers.16.attention.wo.weight', 'layers.16.feed_forward.w1.weight', 'layers.16.feed_forward.w2.weight', 'layers.16.feed_forward.w3.weight', 'layers.16.attention_norm.weight', 'layers.16.ffn_norm.weight', 'layers.17.attention.wq.weight', 'layers.17.attention.wk.weight', 'layers.17.attention.wv.weight', 'layers.17.attention.wo.weight', 'layers.17.feed_forward.w1.weight', 'layers.17.feed_forward.w2.weight', 'layers.17.feed_forward.w3.weight', 'layers.17.attention_norm.weight', 'layers.17.ffn_norm.weight', 'layers.18.attention.wq.weight', 'layers.18.attention.wk.weight', 'layers.18.attention.wv.weight', 'layers.18.attention.wo.weight', 'layers.18.feed_forward.w1.weight', 'layers.18.feed_forward.w2.weight', 'layers.18.feed_forward.w3.weight', 'layers.18.attention_norm.weight', 'layers.18.ffn_norm.weight', 'layers.19.attention.wq.weight', 'layers.19.attention.wk.weight', 'layers.19.attention.wv.weight', 'layers.19.attention.wo.weight', 'layers.19.feed_forward.w1.weight', 'layers.19.feed_forward.w2.weight', 'layers.19.feed_forward.w3.weight', 'layers.19.attention_norm.weight', 'layers.19.ffn_norm.weight', 'layers.20.attention.wq.weight', 'layers.20.attention.wk.weight', 'layers.20.attention.wv.weight', 'layers.20.attention.wo.weight', 'layers.20.feed_forward.w1.weight', 'layers.20.feed_forward.w2.weight', 'layers.20.feed_forward.w3.weight', 'layers.20.attention_norm.weight', 'layers.20.ffn_norm.weight', 'layers.21.attention.wq.weight', 'layers.21.attention.wk.weight', 'layers.21.attention.wv.weight', 'layers.21.attention.wo.weight', 'layers.21.feed_forward.w1.weight', 'layers.21.feed_forward.w2.weight', 'layers.21.feed_forward.w3.weight', 'layers.21.attention_norm.weight', 'layers.21.ffn_norm.weight', 'layers.22.attention.wq.weight', 'layers.22.attention.wk.weight', 'layers.22.attention.wv.weight', 'layers.22.attention.wo.weight', 'layers.22.feed_forward.w1.weight', 'layers.22.feed_forward.w2.weight', 'layers.22.feed_forward.w3.weight', 'layers.22.attention_norm.weight', 'layers.22.ffn_norm.weight', 'layers.23.attention.wq.weight', 'layers.23.attention.wk.weight', 'layers.23.attention.wv.weight', 'layers.23.attention.wo.weight', 'layers.23.feed_forward.w1.weight', 'layers.23.feed_forward.w2.weight', 'layers.23.feed_forward.w3.weight', 'layers.23.attention_norm.weight', 'layers.23.ffn_norm.weight', 'layers.24.attention.wq.weight', 'layers.24.attention.wk.weight', 'layers.24.attention.wv.weight', 'layers.24.attention.wo.weight', 'layers.24.feed_forward.w1.weight', 'layers.24.feed_forward.w2.weight', 'layers.24.feed_forward.w3.weight', 'layers.24.attention_norm.weight', 'layers.24.ffn_norm.weight', 'layers.25.attention.wq.weight', 'layers.25.attention.wk.weight', 'layers.25.attention.wv.weight', 'layers.25.attention.wo.weight', 'layers.25.feed_forward.w1.weight', 'layers.25.feed_forward.w2.weight', 'layers.25.feed_forward.w3.weight', 'layers.25.attention_norm.weight', 'layers.25.ffn_norm.weight', 'layers.26.attention.wq.weight', 'layers.26.attention.wk.weight', 'layers.26.attention.wv.weight', 'layers.26.attention.wo.weight', 'layers.26.feed_forward.w1.weight', 'layers.26.feed_forward.w2.weight', 'layers.26.feed_forward.w3.weight', 'layers.26.attention_norm.weight', 'layers.26.ffn_norm.weight', 'layers.27.attention.wq.weight', 'layers.27.attention.wk.weight', 'layers.27.attention.wv.weight', 'layers.27.attention.wo.weight', 'layers.27.feed_forward.w1.weight', 'layers.27.feed_forward.w2.weight', 'layers.27.feed_forward.w3.weight', 'layers.27.attention_norm.weight', 'layers.27.ffn_norm.weight', 'layers.28.attention.wq.weight', 'layers.28.attention.wk.weight', 'layers.28.attention.wv.weight', 'layers.28.attention.wo.weight', 'layers.28.feed_forward.w1.weight', 'layers.28.feed_forward.w2.weight', 'layers.28.feed_forward.w3.weight', 'layers.28.attention_norm.weight', 'layers.28.ffn_norm.weight', 'layers.29.attention.wq.weight', 'layers.29.attention.wk.weight', 'layers.29.attention.wv.weight', 'layers.29.attention.wo.weight', 'layers.29.feed_forward.w1.weight', 'layers.29.feed_forward.w2.weight', 'layers.29.feed_forward.w3.weight', 'layers.29.attention_norm.weight', 'layers.29.ffn_norm.weight', 'layers.30.attention.wq.weight', 'layers.30.attention.wk.weight', 'layers.30.attention.wv.weight', 'layers.30.attention.wo.weight', 'layers.30.feed_forward.w1.weight', 'layers.30.feed_forward.w2.weight', 'layers.30.feed_forward.w3.weight', 'layers.30.attention_norm.weight', 'layers.30.ffn_norm.weight', 'layers.31.attention.wq.weight', 'layers.31.attention.wk.weight', 'layers.31.attention.wv.weight', 'layers.31.attention.wo.weight', 'layers.31.feed_forward.w1.weight', 'layers.31.feed_forward.w2.weight', 'layers.31.feed_forward.w3.weight', 'layers.31.attention_norm.weight', 'layers.31.ffn_norm.weight', 'norm.weight', 'output.weight', 'backbone.positional_embedding', 'backbone.text_projection', 'backbone.logit_scale', 'backbone.visual.class_embedding', 'backbone.visual.positional_embedding', 'backbone.visual.proj', 'backbone.visual.conv1.weight', 'backbone.visual.ln_pre.weight', 'backbone.visual.ln_pre.bias', 'backbone.visual.transformer.resblocks.0.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.0.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.0.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.0.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.0.ln_1.weight', 'backbone.visual.transformer.resblocks.0.ln_1.bias', 'backbone.visual.transformer.resblocks.0.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.0.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.0.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.0.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.0.ln_2.weight', 'backbone.visual.transformer.resblocks.0.ln_2.bias', 'backbone.visual.transformer.resblocks.1.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.1.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.1.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.1.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.1.ln_1.weight', 'backbone.visual.transformer.resblocks.1.ln_1.bias', 'backbone.visual.transformer.resblocks.1.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.1.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.1.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.1.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.1.ln_2.weight', 'backbone.visual.transformer.resblocks.1.ln_2.bias', 'backbone.visual.transformer.resblocks.2.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.2.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.2.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.2.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.2.ln_1.weight', 'backbone.visual.transformer.resblocks.2.ln_1.bias', 'backbone.visual.transformer.resblocks.2.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.2.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.2.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.2.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.2.ln_2.weight', 'backbone.visual.transformer.resblocks.2.ln_2.bias', 'backbone.visual.transformer.resblocks.3.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.3.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.3.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.3.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.3.ln_1.weight', 'backbone.visual.transformer.resblocks.3.ln_1.bias', 'backbone.visual.transformer.resblocks.3.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.3.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.3.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.3.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.3.ln_2.weight', 'backbone.visual.transformer.resblocks.3.ln_2.bias', 'backbone.visual.transformer.resblocks.4.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.4.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.4.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.4.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.4.ln_1.weight', 'backbone.visual.transformer.resblocks.4.ln_1.bias', 'backbone.visual.transformer.resblocks.4.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.4.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.4.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.4.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.4.ln_2.weight', 'backbone.visual.transformer.resblocks.4.ln_2.bias', 'backbone.visual.transformer.resblocks.5.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.5.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.5.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.5.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.5.ln_1.weight', 'backbone.visual.transformer.resblocks.5.ln_1.bias', 'backbone.visual.transformer.resblocks.5.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.5.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.5.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.5.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.5.ln_2.weight', 'backbone.visual.transformer.resblocks.5.ln_2.bias', 'backbone.visual.transformer.resblocks.6.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.6.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.6.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.6.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.6.ln_1.weight', 'backbone.visual.transformer.resblocks.6.ln_1.bias', 'backbone.visual.transformer.resblocks.6.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.6.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.6.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.6.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.6.ln_2.weight', 'backbone.visual.transformer.resblocks.6.ln_2.bias', 'backbone.visual.transformer.resblocks.7.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.7.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.7.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.7.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.7.ln_1.weight', 'backbone.visual.transformer.resblocks.7.ln_1.bias', 'backbone.visual.transformer.resblocks.7.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.7.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.7.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.7.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.7.ln_2.weight', 'backbone.visual.transformer.resblocks.7.ln_2.bias', 'backbone.visual.transformer.resblocks.8.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.8.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.8.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.8.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.8.ln_1.weight', 'backbone.visual.transformer.resblocks.8.ln_1.bias', 'backbone.visual.transformer.resblocks.8.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.8.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.8.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.8.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.8.ln_2.weight', 'backbone.visual.transformer.resblocks.8.ln_2.bias', 'backbone.visual.transformer.resblocks.9.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.9.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.9.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.9.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.9.ln_1.weight', 'backbone.visual.transformer.resblocks.9.ln_1.bias', 'backbone.visual.transformer.resblocks.9.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.9.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.9.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.9.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.9.ln_2.weight', 'backbone.visual.transformer.resblocks.9.ln_2.bias', 'backbone.visual.transformer.resblocks.10.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.10.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.10.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.10.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.10.ln_1.weight', 'backbone.visual.transformer.resblocks.10.ln_1.bias', 'backbone.visual.transformer.resblocks.10.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.10.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.10.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.10.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.10.ln_2.weight', 'backbone.visual.transformer.resblocks.10.ln_2.bias', 'backbone.visual.transformer.resblocks.11.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.11.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.11.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.11.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.11.ln_1.weight', 'backbone.visual.transformer.resblocks.11.ln_1.bias', 'backbone.visual.transformer.resblocks.11.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.11.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.11.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.11.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.11.ln_2.weight', 'backbone.visual.transformer.resblocks.11.ln_2.bias', 'backbone.visual.transformer.resblocks.12.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.12.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.12.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.12.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.12.ln_1.weight', 'backbone.visual.transformer.resblocks.12.ln_1.bias', 'backbone.visual.transformer.resblocks.12.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.12.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.12.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.12.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.12.ln_2.weight', 'backbone.visual.transformer.resblocks.12.ln_2.bias', 'backbone.visual.transformer.resblocks.13.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.13.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.13.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.13.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.13.ln_1.weight', 'backbone.visual.transformer.resblocks.13.ln_1.bias', 'backbone.visual.transformer.resblocks.13.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.13.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.13.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.13.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.13.ln_2.weight', 'backbone.visual.transformer.resblocks.13.ln_2.bias', 'backbone.visual.transformer.resblocks.14.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.14.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.14.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.14.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.14.ln_1.weight', 'backbone.visual.transformer.resblocks.14.ln_1.bias', 'backbone.visual.transformer.resblocks.14.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.14.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.14.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.14.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.14.ln_2.weight', 'backbone.visual.transformer.resblocks.14.ln_2.bias', 'backbone.visual.transformer.resblocks.15.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.15.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.15.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.15.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.15.ln_1.weight', 'backbone.visual.transformer.resblocks.15.ln_1.bias', 'backbone.visual.transformer.resblocks.15.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.15.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.15.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.15.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.15.ln_2.weight', 'backbone.visual.transformer.resblocks.15.ln_2.bias', 'backbone.visual.transformer.resblocks.16.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.16.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.16.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.16.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.16.ln_1.weight', 'backbone.visual.transformer.resblocks.16.ln_1.bias', 'backbone.visual.transformer.resblocks.16.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.16.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.16.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.16.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.16.ln_2.weight', 'backbone.visual.transformer.resblocks.16.ln_2.bias', 'backbone.visual.transformer.resblocks.17.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.17.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.17.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.17.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.17.ln_1.weight', 'backbone.visual.transformer.resblocks.17.ln_1.bias', 'backbone.visual.transformer.resblocks.17.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.17.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.17.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.17.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.17.ln_2.weight', 'backbone.visual.transformer.resblocks.17.ln_2.bias', 'backbone.visual.transformer.resblocks.18.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.18.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.18.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.18.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.18.ln_1.weight', 'backbone.visual.transformer.resblocks.18.ln_1.bias', 'backbone.visual.transformer.resblocks.18.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.18.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.18.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.18.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.18.ln_2.weight', 'backbone.visual.transformer.resblocks.18.ln_2.bias', 'backbone.visual.transformer.resblocks.19.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.19.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.19.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.19.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.19.ln_1.weight', 'backbone.visual.transformer.resblocks.19.ln_1.bias', 'backbone.visual.transformer.resblocks.19.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.19.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.19.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.19.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.19.ln_2.weight', 'backbone.visual.transformer.resblocks.19.ln_2.bias', 'backbone.visual.transformer.resblocks.20.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.20.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.20.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.20.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.20.ln_1.weight', 'backbone.visual.transformer.resblocks.20.ln_1.bias', 'backbone.visual.transformer.resblocks.20.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.20.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.20.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.20.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.20.ln_2.weight', 'backbone.visual.transformer.resblocks.20.ln_2.bias', 'backbone.visual.transformer.resblocks.21.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.21.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.21.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.21.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.21.ln_1.weight', 'backbone.visual.transformer.resblocks.21.ln_1.bias', 'backbone.visual.transformer.resblocks.21.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.21.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.21.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.21.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.21.ln_2.weight', 'backbone.visual.transformer.resblocks.21.ln_2.bias', 'backbone.visual.transformer.resblocks.22.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.22.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.22.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.22.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.22.ln_1.weight', 'backbone.visual.transformer.resblocks.22.ln_1.bias', 'backbone.visual.transformer.resblocks.22.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.22.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.22.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.22.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.22.ln_2.weight', 'backbone.visual.transformer.resblocks.22.ln_2.bias', 'backbone.visual.transformer.resblocks.23.attn.in_proj_weight', 'backbone.visual.transformer.resblocks.23.attn.in_proj_bias', 'backbone.visual.transformer.resblocks.23.attn.out_proj.weight', 'backbone.visual.transformer.resblocks.23.attn.out_proj.bias', 'backbone.visual.transformer.resblocks.23.ln_1.weight', 'backbone.visual.transformer.resblocks.23.ln_1.bias', 'backbone.visual.transformer.resblocks.23.mlp.c_fc.weight', 'backbone.visual.transformer.resblocks.23.mlp.c_fc.bias', 'backbone.visual.transformer.resblocks.23.mlp.c_proj.weight', 'backbone.visual.transformer.resblocks.23.mlp.c_proj.bias', 'backbone.visual.transformer.resblocks.23.ln_2.weight', 'backbone.visual.transformer.resblocks.23.ln_2.bias', 'backbone.visual.ln_post.weight', 'backbone.visual.ln_post.bias', 'backbone.token_embedding.weight', 'backbone.ln_final.weight', 'backbone.ln_final.bias'], unexpected_keys=[])\n",
            "Reloaded SentencePiece model from ./data/weights/tokenizer.model\n",
            "#words: 32000 - BOS ID: 1 - EOS ID: 2\n",
            "split:  test\n",
            "total_items:  4241\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "4300it [06:31, 10.97it/s]\n",
            "overall accuracy:  85.80523461447773\n",
            "/content/ADEM-VL/eval_sqa.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  res_pd.loc[index, 'no_context'] = True if (not row['hint'] and not row['image']) else False\n",
            "/content/ADEM-VL/eval_sqa.py:51: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  res_pd.loc[index, 'has_text'] = True if row['hint'] else False\n",
            "/content/ADEM-VL/eval_sqa.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  res_pd.loc[index, 'has_image'] = True if row['image'] else False\n",
            "/content/ADEM-VL/eval_sqa.py:53: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  res_pd.loc[index, 'has_text_image'] = True if (row['hint'] and row['image']) else False\n",
            "/content/ADEM-VL/eval_sqa.py:57: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  res_pd.loc[index, 'pred'] = pred\n",
            "/content/ADEM-VL/eval_sqa.py:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  res_pd.loc[index, 'true_false'] = (label == pred)\n",
            "{'acc_natural': '86.06', 'acc_social': '83.46', 'acc_language': '87.18', 'acc_has_text': '85.00', 'acc_has_image': '79.72', 'acc_no_context': '89.06', 'acc_grade_1_6': '86.71', 'acc_grade_7_12': '84.18', 'acc_average': '85.81'}\n"
          ]
        }
      ]
    }
  ]
}